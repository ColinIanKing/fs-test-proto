#!/usr/bin/python
#
# Copyright (C) 2014 Canonical, Ltd.
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.
#
# Author: Colin Ian King <colin.king@canonical.com>
#
import os
import sys
import stat
import json
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import pylab
import re
import gzip
import string
import errno
import textwrap

if len(sys.argv) < 2:
	sys.exit('Usage: %s results-path' % sys.argv[0])

if not os.path.exists(sys.argv[1]):
	sys.exit('Path %s not found' % sys.argv[1])

import os, errno

#
#  Create all dirs in a given path
#
def mkdirp(path):
	try:
		os.makedirs(path)
	except OSError as exc:
		if exc.errno == errno.EEXIST and os.path.isdir(path):
			pass
		else: raise

#
#  Load sysinfo
#
def sysinfo_load(file):
	sysinfo = {}

	if os.path.isfile(file):
		f = open(file, 'r')
	else:
		f = gzip.open(file + '.gz')

	for line in f.read().splitlines():
		if ':' in line:
			[key, val] = line.split(':',1)
			sysinfo[key] = val.lstrip()
	f.close()

	return sysinfo

#
#  Text wrap
#
def text_wrap(text, len):
	return '\n'.join(textwrap.wrap(text, len))

job_config_cache = {}

#
#  Get and cache job configuration data
#
def job_config_get(job):
	config = []
	keys = [ 'Label', 'Units', 'Scale', 'JsonPath' ]
	if job in job_config_cache:
		return job_config_cache[job]
	else:
		f = open(os.path.join('jobs', job + '.conf'), 'r')
		for line in f.read().splitlines():
			if line[:1] == '#':
				continue
			info = {}
			fields = line.split(',', 4)
			if ',' in line:
				for i in range(0, len(fields)):
					info[keys[i]] = fields[i].lstrip().rstrip()

			config.append(info)
		f.close()
		job_config_cache[job] = config
		return config

#
#  Variants of keys?
#
def keys_variant_collection(part_stats):
	skipkeys = [ 'Time', 'Kernel-version', 'Full Date']
	keys = []
	for stats in part_stats:
		for (cur, sysinfo1, fio_stats) in stats:
			for (cur, sysinfo2, fio_stats) in stats:
				for key in sysinfo2:
					try:
						if not key in keys and not key in skipkeys and sysinfo1[key] != sysinfo2[key]:
							keys.append(key)
					except:
						pass
	return keys

#
#  Remove trailing comma and spaces from label
#
def clean_label(label):
	str = label
	while str.endswith(' ') or str.endswith(','):
		str = str[:-2]
	return str

#
#  Take an array of words and return words as a comma separated string
#
def array_to_str(array):
	return ', '.join(array)

def sysinfo_keys_to_label(sysinfo, keys):
	label = []
	for key in keys:
		label.append(sysinfo[key])

	return ', '.join(label)

#
#  Convert a string into a filename containing letters and digits
#
def str_to_filename(str):
	filename = ''
	for c in str.lower():
		if c in string.ascii_letters or c in string.digits:
			filename += c
		else:
			filename += '-'

	return filename

#
#  Save a png to file
#
def save_png(path, title, fig):
	filename = os.path.join(path, str_to_filename(title) + ".png")
	fig.savefig(filename, dpi=70, transparent=True)
	print "Graphed " + filename
	return filename

#
#  Very simple horizontal bar chart
#
def bar_horiz(html_subdirs, root_path, keys, config, collection):
	title = config['Label']
	units = config['Units']

	print "Graphing " + title

	#
	#  Filename based on title..
	#
	filename = str_to_filename(title)
	newpath = os.path.join(root_path, filename)
	mkdirp(newpath)

	#
	#  Sort collection on xlabel
	#
	collection = sorted(collection, key=lambda d: d[3])

	#
	#  Collect up end labels, see how many there are and
	#  this allows us to determine how to color the bars
	#
	endlabels = []
	for (subtitle, xlabels, yvalues, xlabel) in collection:
		for l in xlabels:
			end = l.split()[::-1][0]
			if not end in endlabels:
				endlabels.append(end)
	colors ='rgbwmcrgbwnc'[0:len(endlabels)]

	n = 0
	for c in collection:
		(subtitle, xlabels, yvalues, xlabel) = c
		nbars = len(yvalues)
		fig, plot = plt.subplots(1, figsize=(9, 1 + (0.35 * nbars)))

		xvalues = []
		for x in xlabels:
			xvalues.append(xlabels.index(x))

		ymin = 0 # min(yvalues)
		ymax = max(yvalues)
		if ymax - ymin == 0:
			ymax = 1
		#
		#  And sort values in x order (sigh)
		#
		indexes = np.argsort(xlabels)[::-1]
		xlabels = [ xlabels[i] for i in indexes ]
		yvalues = [ yvalues[i] for i in indexes ]
		pos = np.arange(len(xlabels)) + 0.5

		plot.set_xlim(ymin, ymax)
		plot.set_ylim(0, len(xlabels))
		plot.xaxis.grid(True, which='both')
		plot.yaxis.grid(True, which='both')
		plot.barh(pos, yvalues, align='center', color=colors)
		plt.title(xlabel + ": " + title + " (" + units + ")" + "\n" + subtitle)

		plt.yticks(pos, xlabels)
		plt.tight_layout()
		save_png(newpath, title + "-%3.3d" % n, fig)
		plt.close()
		n += 1

	html_subdirs.append(generate_html([], root_path, newpath, filename, title))

#
#  Very simple graph plot
#
def plot_horiz(html_subdirs, path, keys, config, collection):
	title = config['Label']
	units = config['Units']

	print "Graphing " + title

	fig, plot = plt.subplots(figsize=(8, 6))

	#
	#  Determine all the xlabels in the collection
	#
	all_xlabels = []
	ymax = 0
	ymin = 1e12
	for c in collection:
		(subtitle, xlabels, yvalues, xlabel) = c
		for xlabel in xlabels:
			if not xlabel in all_xlabels:
				all_xlabels.append(xlabel)
		ymax = max(ymax, max(yvalues))
		ymin = min(ymin, min(yvalues))

	print "Graphing " + title

	(subtitle, xlabels, yvalues, xlabel) = collection[0]

	for c in collection:
		(subtitle, xlabels, yvalues, xlabel) = c
		xvalues = []
		for x in xlabels:
			xvalues.append(all_xlabels.index(x))
		#
		#  And sort vales in x order (sigh)
		#
		indexes = np.argsort(xvalues)
		xvalues = [ xvalues[i] for i in indexes ]
		yvalues = [ yvalues[i] for i in indexes ]
		plot.plot(xvalues, yvalues, label = xlabel)
		plot.grid(True)
		plot.set_xlabel(subtitle)
		plot.set_ylabel(title + " (" + units + ")")
		plot.set_ylim(ymin * 0.80, ymax * 1.05)
		plot.set_xlim(0, len(all_xlabels) -1)

	plt.xticks(np.arange(len(all_xlabels)), all_xlabels, rotation=90)
	plt.title(title)
	plt.legend(loc = 6, prop={'size':10})
	plt.tight_layout()
	save_png(path, title, fig)
	plt.close()

def plot_samples_by_filesystem_cols(path, keys, title, data):
	title_keys = ['Job', 'Filesystem', 'Kernel-release']
	cols = []
	for (values, sysinfo) in data:
		if not sysinfo['Filesystem'] in cols:
			cols.append(sysinfo['Filesystem'])

	nplots = len(data)
	ncols = len(cols)
	nrows = nplots / ncols
	max_y = 0
	#
	#  Calculate some simple stats
	#
	for (values, sysinfo) in data:
		total_y = 0
		total_n = 0
		for (x, y) in values:
			if len(y) > 0:
				max_y = max(max_y, max(y))
				total_y += sum(y)
				total_n += len(y)
		if total_n == 0:
			sysinfo['mean_y'] = 0
		else:
			sysinfo['mean_y'] = float(total_y) / total_n

	subtitle = ""
	for k in title_keys:
		if not k in keys:
			subtitle += sysinfo[k] + ", "

	for j in range(0, nrows):
		fig, plots = plt.subplots(nrows=2, ncols=ncols, figsize=(5 * ncols, 6))
		for i in range(0, ncols):
			index = (i * nrows) + j
			(values, sysinfo) = data[index]

			for (x, y) in values:
				if len(x) > 0:
					max_x = max(x)
				else:
					max_x = 0
				if len(y) > 0:
					max_y = max(y)
				else:
					max_y = 0

				plots[0][i].plot(x, y, '.', color='black')
				plots[0][i].set_xlabel('Sample #')
				plots[0][i].set_ylabel(title)
				plots[0][i].set_title(title + '\n' + text_wrap(subtitle + sysinfo_keys_to_label(sysinfo, keys), 30))
				plots[0][i].yaxis.grid(True, which='both')
				plots[0][i].axes.get_xaxis().set_ticks([])

				if max_x > 0 and max_y > 0:
					heatmap, yedges, xedges = np.histogram2d(y, x, bins=(16,32))
					extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]
					plots[1,i].imshow(heatmap, extent=extent, aspect='auto', origin="lower")
				plots[1,i].set_xlabel('Sample #')
				plots[1,i].set_ylabel(title)
				plots[1,i].set_title("Heatmap: " + title + '\n' + text_wrap(subtitle + sysinfo_keys_to_label(sysinfo, keys), 30))
				plots[1][i].axes.get_xaxis().set_ticks([])

			yy = sysinfo['mean_y']
			fs = sysinfo['Filesystem']

			#
			#  Drawn "mean" line
			#
			x = [0, max_x ]
			y = [yy, yy]
			plots[0][i].plot(x, y, label='Mean ' + '%.4f' % yy, color='red')
			plots[0][i].legend(loc = 1, prop={'size':10})

		plt.tight_layout()
		save_png(path, title + '-2-graphs-%3.3d' % j, fig)
		plt.close()
	return

def hist_samples_by_filesystem_cols(path, keys, title, data):
	title_keys = ['Job', 'Filesystem', 'Kernel-release']

	cols = []
	rows = []
	for (values, sysinfo) in data:
		if not sysinfo['Filesystem'] in cols:
			cols.append(sysinfo['Filesystem'])
		if not sysinfo['Job'] in rows:
			rows.append(sysinfo['Job'])
	rows = sorted(rows)
	cols = sorted(cols)[::-1]
	nrows = len(rows)
	ncols = len(cols)

	subtitle = ""
	for k in title_keys:
		if not k in keys:
			subtitle += sysinfo[k] + ", "

	mean_y = np.zeros((nrows, ncols))
	for (values, sysinfo) in data:
		col = cols.index(sysinfo['Filesystem'])
		row = rows.index(sysinfo['Job'])
		total_y = 0
		total_n = 0
		for (x, y) in values:
			total_y += sum(y)
			total_n += len(y)

		if total_n != 0:
			mean_y[row,col] = float(total_y) / total_n
		else:
			mean_y[row,col] = 0

	fig, plots = plt.subplots(nrows, figsize=(8, 1.5 * nrows + 1))
	pos = np.arange(ncols)
	for row in rows:
		i = rows.index(row)
		plots[i].xaxis.grid(True, which='both')
		plots[i].barh(pos, mean_y[i], align='center') #, color=colors)
		plots[i].set_title(title + '\n' + subtitle + " " + row)
		fig.sca(plots[i])
		plt.yticks(pos, cols)

	plt.tight_layout()
	save_png(path, title + '-1-histogram', fig)
	plt.close()
	return

def plot_samples(path, keys, title, data):
	nplots = len(data)
	title_keys = ['Job', 'Filesystem', 'Kernel-release']

	fig, plots = plt.subplots(nrows=nplots, figsize=(7, nplots * 3))

	i = 0
	max_y = 0
	#
	#  Calculate some simple stats
	#
	for (values, sysinfo) in data:
		total_y = 0
		total_n = 0
		for (x, y) in values:
			max_y = max(max_y, max(y))
			total_y += sum(y)
			total_n += len(y)
		if total_n == 0:
			sysinfo['mean_y'] = 0
		else:
			sysinfo['mean_y'] = float(total_y) / total_n

	subtitle = ""
	for k in title_keys:
		if not k in keys:
			subtitle += sysinfo[k] + ", "

	for (values, sysinfo) in data:
		fs = sysinfo['Filesystem']

		for (x, y) in values:
			max_x = max(x)
			plots[i].plot(x, y, '.', color='black')
			plots[i].set_xlabel('Sample #')
			plots[i].set_ylabel(title)
			plots[i].set_title(title + ', ' + subtitle + sysinfo_keys_to_label(sysinfo, keys))
			plots[i].set_xlim(0, max_x)
			plots[i].yaxis.grid(True, which='both')

		yy = sysinfo['mean_y']
		fs = sysinfo['Filesystem']
		x = [0, max_x ]
		y = [yy, yy]
		plots[i].plot(x, y, label='Mean ' + '%.4f' % yy, color='red')
		plots[i].legend(loc = 2)
		i += 1

	plt.tight_layout()
	save_png(path, title, fig)
	plt.close()
	return

#
#  Traverse a path to find json values
#
def get_json_val(json_data, fullpath):
	path = fullpath.split('.')
	j = json_data
	try:
		for p in path:
			if p.isdigit():
				j = j[int(p)]
			else:
				j = j[p]
		return j
 	except:
		return 0


#
# Do statistic data gathering
#
def compare_stats(html_subdirs, root_path, graph_func, stats_collection, collection_keys):
	#
	# Determine which jobs were run
	# and hence all the unique configs
	#
	unique_configs = []
	keys = keys_variant_collection(stats_collection)

	for stats in stats_collection:
		for (cur, sysinfo, fio_stats) in stats:
			job = sysinfo['Job']
			config = job_config_get(job)
			for c in config:
				if not c in unique_configs:
					unique_configs.append(c)

	#
	#  The following are config values that we may be
	#  interested in in labelling
	#
	sysinfokeys = [ 'Blocksize', 'Filesystem', 'Device', 'Hardware',
		       	'Job', 'Date', 'Kernel-release', 'Filesize', 'Processor', 'IO Scheduler' ]

	#
	#  Step #1, find all the different sysinfo fields that change
	#  across all the collections and tests
	#
	sysinfovals = {}
	collection_fields_vary = []
	for config in unique_configs:
		for stats in stats_collection:
			for (cur, sysinfo, fio_stats) in stats:
				if config in job_config_get(sysinfo['Job']):
					for c in sysinfokeys:
						try:
							if not sysinfo[c] in sysinfovals[c]:
								sysinfovals[c].append(sysinfo[c])
						except:
							sysinfovals[c] = [ sysinfo[c] ]

	for s in sysinfovals:
		if len(sysinfovals[s]) > 1:
			collection_fields_vary.append(s)

	#
	#  Step #2, gather the data
	#
	for config in unique_configs:
		sysinfovals = {}
		collection = []

		#
		#  find all the different sysinfo fields that change and don't
		#  change across the tests
		#
		for stats in stats_collection:
			for (cur, sysinfo, fio_stats) in stats:
				if config in job_config_get(sysinfo['Job']):
					for c in sysinfokeys:
						try:
							if not sysinfo[c] in sysinfovals[c]:
								sysinfovals[c].append(sysinfo[c])
						except:
							sysinfovals[c] = [sysinfo[c]]

		sysinfo_fields_vary = []
		sysinfo_fields_static = []

		for s in sysinfovals:
			if len(sysinfovals[s]) > 1:
				if not s in collection_keys:
					sysinfo_fields_vary.append(s)
			else:
				sysinfo_fields_static.append(s)

		label = config['Label']
		units = config['Units']
		scale = float(config['Scale'])
		jsonpath = config['JsonPath']

		subtitle = []
		for f in sysinfo_fields_static:
			subtitle.append(sysinfo[f])

		for stats in stats_collection:
			yvalues = []
			xlabels = []
			for (cur, sysinfo, fio_stats) in stats:
				if config in job_config_get(sysinfo['Job']):
					val = get_json_val(fio_stats, jsonpath)
					yvalues.append(val / scale)
					xlabel = ""
					for f in sysinfo_fields_vary:
						xlabel += sysinfo[f] + ", "

					xlabels.append(clean_label(xlabel))

					for c in sysinfokeys:
						try:
							if not sysinfo[c] in sysinfovals[c]:
								sysinfovals[c].append(sysinfo[c])
						except:
							sysinfovals[c] = [sysinfo[c]]
				xlabel = []
				for f in collection_keys:
					xlabel.append(sysinfo[f])
			#
			# Add them to the subplot collection
			#
			if len(xlabels) > 0:
				collection.append((array_to_str(subtitle), xlabels, yvalues, array_to_str(xlabel)))

		graph_func(html_subdirs, root_path, keys, config, collection)

#
#  Load in fio-stats.json and sysinfo.log for the given tests
#
def collect_stats(path):
	stats = []
	for cur, _dirs, files in os.walk(path):
		head, tail = os.path.split(cur)
		if tail == "stats":
			sysinfo = sysinfo_load(os.path.join(cur, 'sysinfo.log'))
			job_config_get(sysinfo['Job'])
			try:
				f = open(os.path.join(cur, 'fio-stats.json'), 'r')
				fio_stats = json.load(f)
				f.close()
				stats.append((cur, sysinfo, fio_stats))
			except:
				print "Failed to read fio-stats.json for " + cur
	return stats


def find_matching_stats(stats, criteria):
	matches = []
	for (cur, sysinfo, fio_stats) in stats:
		keep = True
		for (key, values) in criteria:
			if values != []:
				found = False
				for value in values:
					if key in sysinfo and sysinfo[key] == value:
						found = True
						break
				keep = keep and found
		if keep:
			matches.append((cur, sysinfo, fio_stats))

	return matches

def find_unique_key_value_stats(stats, criteria, key):
	values = []
	matches = find_matching_stats(stats, criteria)
	for (cur, sysinfo, fio_stats) in matches:
		if not sysinfo[key] in values:
			values.append(sysinfo[key])

	print "Found %d unique stats" % len(values)
	return values

def compare_samples(path, stats, keys, title, pattern):
	regex = re.compile(pattern)
	data = []
	titles = []
	for (cur, sysinfo, fio_stats) in stats:
		values = []
		for l in sorted(os.listdir(cur)):
			x = []
			y = []
			for m in [regex.search(l)]:
				if m:
					filename = os.path.join(cur, l)
					if filename[-3:] == '.gz':
						f = gzip.open(filename)
					else:
						f = open(filename, 'r')
					for line in f.read().splitlines():
						s = line.split(',')
						x.append(int(s[0]))
						y.append(int(s[1]))
					f.close()
					values.append((x,y))
		data.append((values, sysinfo))
		titles.append(sysinfo_keys_to_label(sysinfo, keys))
	#
	# Sort based on the chart titles
	#
	indexes = np.argsort(titles)
	titles = [ titles[i] for i in indexes ]
	data = [ data[i] for i in indexes ]

	if len(data):
		plot_samples_by_filesystem_cols(path, keys, title, data)
		hist_samples_by_filesystem_cols(path, keys, title, data)
	else:
		print "No relevant data can be found"

def compare_stats_ordered_traverse(graph_func, stats, group_by_keys, collection_keys, traversed_stats):
	if (len(group_by_keys) <= 1):
		sortstats = sorted(stats, key = lambda d: d[1][group_by_keys[0]])
		traversed_stats.append(sortstats)
		return
	#
	#  For all key values, partition into "collections"
	#  where the key value is identical and sub-divide
	#  or graph on next key value
	#
	k = group_by_keys[0]
	suborder_by_keys = group_by_keys[1:len(group_by_keys)]

	if not k in collection_keys:
		compare_stats_ordered_traverse(graph_func, stats, suborder_by_keys, collection_keys, traversed_stats)
	else:
		values = []
		for (cur, sysinfo, fio_stats) in stats:
			if not sysinfo[k] in values:
				values.append(sysinfo[k])

		for v in values:
			substats = []
			for s in stats:
				(cur, sysinfo, fio_stats) = s
				if sysinfo[k] == v:
					substats.append(s)

			compare_stats_ordered_traverse(graph_func, substats, suborder_by_keys, collection_keys, traversed_stats)


def compare_stats_ordered(html_subdirs, path, graph_func, stats, group_by_keys, collection_keys):
	traversed_stats = []
	compare_stats_ordered_traverse(graph_func, stats, group_by_keys, collection_keys, traversed_stats)
	compare_stats(html_subdirs, path, graph_func, traversed_stats, collection_keys)

#
#  generate a html file containing links to graphs
#
def generate_html(html_subdirs, root_path, sub_path, html_file, title):
	page = '''
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <meta content="text/html; charset=ISO-8859-1"
 http-equiv="content-type">
  <title>''' + title + '''</title>
  <h2><center>''' + title + '''</center></h2>'''


	page += '''
</head>
<body>
<center>
  <table width=100% cellspacing="2">
'''
	colors = [ "#ffffff", "#e0f0ff" ]
	i = 0
	for (t, f) in sorted(html_subdirs, key=lambda d: d[0]):
		common = os.path.commonprefix([sub_path, f])
		relname = f[len(common)+1:]
		page += '\n <tr><td><center><a href="' + relname + '">' + t + '</a></center></td></tr>'

	for f in sorted(os.listdir(sub_path)):
		(filename, extension) = os.path.splitext(f)
		if extension == ".png":
			page += '\n <tr><td bgcolor="' + colors[i % 2] + '"><center><img src=' + f + '></center></td></tr>'
			i += 1
	page += '''
  </table>
</center>
</body>
</html>'''
	mkdirp(sub_path)
	filename = os.path.join(sub_path, "index.html")
	f = open(filename, 'w')
	f.write(page)
	f.close()

	return (title, filename)

#
#  Histogram of fs + tests on a given day
#
def daily_tests_histogram(html_dirs, root_path, date):
	tests = find_matching_stats(stats, [ ('Date', date), ('Job', []), ('Filesystem', []), ('Kernel-release', []), ('IO Scheduler', [])])
	#
	# Compare filesystems
	#
	newpath = os.path.join(root_path, 'daily-fio-histograms-by-filesystem')
	mkdirp(newpath)
	html_subdirs = []
	compare_stats_ordered(html_subdirs, newpath, bar_horiz, tests, ['IO Scheduler', 'Job', 'Filesystem', 'Date' ], ['Job'])
	html_dirs.append(generate_html(html_subdirs, root_path, newpath, 'daily-tests-histogram', 'Histograms ' + ', '.join(date)))

#
#  Samples of fs on a given day
#
def daily_tests_samples(html_dirs, root_path, date):
	tests = find_matching_stats(stats, [ ('Date', date), ('Job', []), ('Filesystem', []), ('Kernel-release', [])])
	keys = keys_variant_collection([tests])

	#
	#  Process raw samples from fio logs
	#
	graphs = [
		('Bandwidth KB per sec',	'job.*_bw\.log*', 'daily-fio-stats-bandwidth'),
		('I/O ops per sec', 'job.*_iops\.log*', 'daily-fio-stats-io-ops'),
		('I/O Latency (us)', 'job.*_lat\.log*', 'daily-fio-stats-io-latency'),
		('I/O Completion Latency (us) ', 'job.*_clat\.log*', 'daily-fio-stats-clatency') ]

	for g in graphs:
		newpath = os.path.join(root_path, g[2])
		mkdirp(newpath)
		compare_samples(newpath, tests, keys, g[0], g[1])
		html_dirs.append(generate_html([], root_path, newpath, g[2], g[0] + ": " + ', '.join(date)))

#
#  Plot trends over time
#
def overtime_trends(html_dirs, root_path):
	newpath = os.path.join(root_path, 'overtime-trends')
	mkdirp(newpath)

	kernel_release = []
	file_systems = []
	date = []

	jobs = find_unique_key_value_stats(stats, [ ('Date', date), ('Job', []), ('Filesystem', file_systems), ('Kernel-release', kernel_release)], 'Job')

	html_subdirs = []
	for job in jobs:
		print "Generating trends for job " + job
		jobpath = os.path.join(newpath, job)
		mkdirp(jobpath)
		tests = find_matching_stats(stats, [ ('Date', date), ('Job', [job]), ('Filesystem', file_systems), ('Kernel-release', kernel_release )])
		compare_stats_ordered(html_subdirs, jobpath, plot_horiz, tests, ['Job', 'Filesystem', 'Date'], ['Filesystem', 'Job'])
		html_subdirs.append(generate_html([], newpath, jobpath, 'overtime-trends', job + ": " + 'trends over time'))

	html_dirs.append(generate_html(html_subdirs, newpath, newpath, 'overtime-trends', 'All trends over time'))

#
#  Gather up stats
#
stats = []

print "Reading data.."
for i in range(1, len(sys.argv)):
	stats += collect_stats(sys.argv[i])

tmpdir = "/tmp/test"
print "results in " + tmpdir

html_dirs = []
#daily_tests_histogram(html_dirs, tmpdir, [ '2014-07-01', '2014-07-02'])
daily_tests_samples(html_dirs, tmpdir, ['2014-07-01', '2014-07-02'])
#overtime_trends(html_dirs, tmpdir)

generate_html(html_dirs, tmpdir, tmpdir, 'results', 'All results')






