#!/usr/bin/python
#
# Copyright (C) 2014 Canonical, Ltd.
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.
#
# Author: Colin Ian King <colin.king@canonical.com>
#
import os
import sys
import stat
import json
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import pylab
import re
import gzip
import string
import errno
import os
import textwrap

if len(sys.argv) < 2:
	sys.exit('Usage: %s results-path' % sys.argv[0])

if not os.path.exists(sys.argv[1]):
	sys.exit('Path %s not found' % sys.argv[1])

#
#  fio data log mappings
#
#
samples_info = [
	#  key		human readable title			regex on log name	generated filename path
	('Bandwidth',	'Bandwidth KB per second (by test)',	'job.*_bw\.log*', 	'daily-fio-stats-bandwidth'),
	('IOops',	'I/O ops per second (by test)', 	'job.*_iops\.log*', 	'daily-fio-stats-io-ops'),
	('IOLatency',	'I/O Latency (us) (by test)', 		'job.*_lat\.log*', 	'daily-fio-stats-io-latency'),
	('IOCLatencty',	'I/O Completion Latency (us) (by test)','job.*_clat\.log*', 	'daily-fio-stats-clatency') ]

#
#  Create all dirs in a given path
#
def mkdirp(path):
	try:
		os.makedirs(path)
	except OSError as exc:
		if exc.errno == errno.EEXIST and os.path.isdir(path):
			pass
		else: raise

#
#  Load sysinfo
#
def sysinfo_load(file):
	sysinfo = {}

	if os.path.isfile(file):
		f = open(file, 'r')
	else:
		f = gzip.open(file + '.gz')

	for line in f.read().splitlines():
		if ':' in line:
			[key, val] = line.split(':',1)
			sysinfo[key] = val.lstrip()
	f.close()
	return sysinfo

#
#  Text wrap
#
def text_wrap(text, len):
	return '\n'.join(textwrap.wrap(text, len))

job_config_cache = {}

#
#  Get and cache job configuration data
#
def job_config_get(job):
	config = []
	keys = [ 'Label', 'Units', 'Scale', 'JsonPath' ]
	if job in job_config_cache:
		return job_config_cache[job]
	else:
		f = open(os.path.join('jobs', job + '.conf'), 'r')
		for line in f.read().splitlines():
			if line[:1] == '#':
				continue
			info = {}
			fields = line.split(',', 4)
			if ',' in line:
				for i in range(0, len(fields)):
					info[keys[i]] = fields[i].lstrip().rstrip()

			config.append(info)
		f.close()
		job_config_cache[job] = config
		return config

#
#  Variants of keys?
#
def keys_variant_collection(part_stats):
	skipkeys = [ 'Time', 'Kernel-version', 'Full Date']
	keys = []
	for stats in part_stats:
		for (cur, sysinfo1, fio_stats, samples) in stats:
			for (cur, sysinfo2, fio_stats, samples) in stats:
				for key in sysinfo2:
					try:
						if not key in keys and not key in skipkeys and sysinfo1[key] != sysinfo2[key]:
							keys.append(key)
					except:
						pass
	return keys

#
#  Remove trailing comma and spaces from label
#
def clean_label(label):
	str = label
	while str.endswith(' ') or str.endswith(','):
		str = str[:-2]
	return str

#
#  Take an array of words and return words as a comma separated string
#
def array_to_str(array):
	return ', '.join(array)

#
#  keys in sysinfo to a label
#
def sysinfo_keys_to_label(sysinfo, keys):
	label = []
	for key in keys:
		label.append(sysinfo[key])

	return ', '.join(label)

#
#  Convert a string into a filename containing letters and digits
#
def str_to_filename(str):
	filename = ''
	for c in str.lower():
		if c in string.ascii_letters or c in string.digits:
			filename += c
		else:
			filename += '-'

	return filename

#
#  Save a png to file
#
def save_png(path, title, fig):
	filename = os.path.join(path, str_to_filename(title) + ".png")
	fig.savefig(filename, dpi=70, transparent=True)
	print "Graphed " + filename
	return filename

#
#  Very simple horizontal bar chart
#
def bar_horiz(html_subdirs, root_path, keys, config, collection):
	title = config['Label']
	units = config['Units']
	print "Graphing " + title
	#
	#  Filename based on title..
	#
	filename = str_to_filename(title)
	newpath = os.path.join(root_path, filename)
	mkdirp(newpath)
	#
	#  Sort collection on xlabel
	#
	collection = sorted(collection, key=lambda d: d[3])
	#
	#  Collect up end labels, see how many there are and
	#  this allows us to determine how to color the bars
	#
	endlabels = []
	for (subtitle, xlabels, yvalues, xlabel) in collection:
		for l in xlabels:
			end = l.split()[::-1][0]
			if not end in endlabels:
				endlabels.append(end)
	colors ='rgbwmcrgbwnc'[0:len(endlabels)]

	n = 0
	for c in collection:
		(subtitle, xlabels, yvalues, xlabel) = c
		nbars = len(yvalues)
		fig, plot = plt.subplots(1, figsize=(9, 1 + (0.35 * nbars)))

		xvalues = []
		for x in xlabels:
			xvalues.append(xlabels.index(x))

		ymin = 0 # min(yvalues)
		ymax = max(yvalues)
		if ymax - ymin == 0:
			ymax = 1
		#
		#  And sort values in x order (sigh)
		#
		indexes = np.argsort(xlabels)[::-1]
		xlabels = [ xlabels[i] for i in indexes ]
		yvalues = [ yvalues[i] for i in indexes ]
		pos = np.arange(len(xlabels)) + 0.5

		plot.set_xlim(ymin, ymax)
		plot.set_ylim(0, len(xlabels))
		plot.xaxis.grid(True, which='both')
		plot.yaxis.grid(True, which='both')
		plot.barh(pos, yvalues, align='center', color=colors)
		plt.title(xlabel + ": " + title + " (" + units + ")" + "\n" + subtitle)

		plt.yticks(pos, xlabels)
		plt.tight_layout()
		save_png(newpath, title + "-%3.3d" % n, fig)
		plt.close()
		n += 1

	html_subdirs.append(generate_html([], root_path, newpath, filename, title))

#
#  Very simple graph plot
#
def plot_horiz(html_subdirs, path, keys, config, collection):
	title = config['Label']
	units = config['Units']

	print "Graphing " + title
	fig, plot = plt.subplots(figsize=(8, 6))

	#
	#  Determine all the xlabels in the collection
	#
	all_xlabels = []
	ymax = 0
	ymin = 1e12
	for c in collection:
		(subtitle, xlabels, yvalues, xlabel) = c
		for xlabel in xlabels:
			if not xlabel in all_xlabels:
				all_xlabels.append(xlabel)
		ymax = max(ymax, max(yvalues))
		ymin = min(ymin, min(yvalues))

	for c in collection:
		(subtitle, xlabels, yvalues, xlabel) = c
		xvalues = []
		for x in xlabels:
			xvalues.append(all_xlabels.index(x))
		#
		#  And sort vales in x order (sigh)
		#
		indexes = np.argsort(xvalues)
		xvalues = [ xvalues[i] for i in indexes ]
		yvalues = [ yvalues[i] for i in indexes ]
		plot.plot(xvalues, yvalues, label = xlabel)
		plot.grid(True)
		plot.set_xlabel(subtitle)
		plot.set_ylabel(title + " (" + units + ")")
		plot.set_ylim(ymin * 0.80, ymax * 1.05)
		plot.set_xlim(0, len(all_xlabels) -1)

	plt.xticks(np.arange(len(all_xlabels)), all_xlabels, rotation=90)
	plt.title(title)
	plt.legend(loc = 6, prop={'size':10})
	plt.tight_layout()
	save_png(path, title, fig)
	plt.close()

#
#  plot samples and heatmaps from raw fio data
#
def plot_samples_raw_fio_data(path, keys, title, data):
	title_keys = ['Job', 'Filesystem', 'Kernel-release']
	cols = []
	for (values, sysinfo) in data:
		if not sysinfo['Filesystem'] in cols:
			cols.append(sysinfo['Filesystem'])

	nplots = len(data)
	ncols = len(cols)
	nrows = nplots / ncols
	max_y = 0
	#
	#  Calculate some simple stats
	#
	for (values, sysinfo) in data:
		total_y = 0
		total_n = 0
		for (x, y) in values:
			if len(y) > 0:
				max_y = max(max_y, max(y))
				total_y += sum(y)
				total_n += len(y)
		if total_n == 0:
			sysinfo['mean_y'] = 0
		else:
			sysinfo['mean_y'] = float(total_y) / total_n

	subtitle = ""
	for k in title_keys:
		if not k in keys:
			subtitle += sysinfo[k] + ", "

	for j in range(0, nrows):
		fig, plots = plt.subplots(nrows=2, ncols=ncols, figsize=(5 * ncols, 6))
		for i in range(0, ncols):
			index = (i * nrows) + j
			(values, sysinfo) = data[index]

			for (x, y) in values:
				if len(x) > 0:
					max_x = max(x)
				else:
					max_x = 0
				if len(y) > 0:
					max_y = max(y)
				else:
					max_y = 0

				plots[0][i].plot(x, y, '.', color='black')
				plots[0][i].set_xlabel('Sample #')
				plots[0][i].set_ylabel(title)
				plots[0][i].set_title(title + '\n' + text_wrap(subtitle + sysinfo_keys_to_label(sysinfo, keys), 30))
				plots[0][i].yaxis.grid(True, which='both')
				plots[0][i].axes.get_xaxis().set_ticks([])

				if max_x > 0 and max_y > 0:
					heatmap, yedges, xedges = np.histogram2d(y, x, bins=(16,32))
					extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]
					plots[1,i].imshow(heatmap, extent=extent, aspect='auto', origin="lower")
				plots[1,i].set_xlabel('Sample #')
				plots[1,i].set_ylabel(title)
				plots[1,i].set_title("Heatmap: " + title + '\n' + text_wrap(subtitle + sysinfo_keys_to_label(sysinfo, keys), 30))
				plots[1][i].axes.get_xaxis().set_ticks([])

			yy = sysinfo['mean_y']
			fs = sysinfo['Filesystem']

			#
			#  Drawn "mean" line
			#
			x = [0, max_x ]
			y = [yy, yy]
			plots[0][i].plot(x, y, label='Mean ' + '%.4f' % yy, color='red')
			plots[0][i].legend(loc = 1, prop={'size':10})

		plt.tight_layout()
		save_png(path, title + '-2-graphs-%3.3d' % j, fig)
		plt.close()
	return

def hist_samples_raw_fio_data(path, keys, title, data):
	title_keys = ['Job', 'Filesystem', 'Kernel-release']
	filesystems = []
	ioscheds = []
	for (values, sysinfo) in data:
		if not sysinfo['Filesystem'] in filesystems:
			filesystems.append(sysinfo['Filesystem'])
		if not sysinfo['IO Scheduler'] in ioscheds:
			ioscheds.append(sysinfo['IO Scheduler'])

	filesystems = sorted(filesystems)[::-1]
	ioscheds = sorted(ioscheds)[::-1]
	colors ='rgbwmcrgbwnc'[0:len(filesystems)]

	subtitle = ""
	for k in title_keys:
		if not k in keys:
			subtitle += sysinfo[k] + ", "

	mean_y = []
	ticks = []
	#
	# Inefficient, needs optimising, but normally it's just
	# over 9 or so values, so it's ok for the moment
	#
	for fs in filesystems:
		for io in ioscheds:
			for (values, sysinfo) in data:
				if sysinfo['Filesystem'] == fs and sysinfo['IO Scheduler'] == io:
					total_y = 0
					total_n = 0
					for (x, y) in values:
						total_y += sum(y)
						total_n += len(y)
	
					if total_n != 0:
						mean_y.append(float(total_y) / total_n)
					else:
						mean_y.append(0)
					ticks.append(io + ' ' + fs)

	fig, plot = plt.subplots(1, figsize=(8, 0.5 * len(data) + 1))
	pos = np.arange(len(data))

	plot.xaxis.grid(True, which='both')
	plot.barh(pos, mean_y, align='center', color=colors)
	plot.set_title(title + '\n' + subtitle)
	fig.sca(plot)
	plt.yticks(pos, ticks)

	plt.tight_layout()
	save_png(path, title + '-1-histogram', fig)
	plt.close()
	return

#
#  Traverse a path to find json values
#
def get_json_val(json_data, fullpath):
	path = fullpath.split('.')
	j = json_data
	try:
		for p in path:
			if p.isdigit():
				j = j[int(p)]
			else:
				j = j[p]
		return j
 	except:
		return 0


#
# Do statistic data gathering
#
def graph_stats(html_subdirs, root_path, graph_func, stats_collection, collection_keys):
	#
	# Determine which jobs were run
	# and hence all the unique configs
	#
	unique_configs = []
	keys = keys_variant_collection(stats_collection)

	for stats in stats_collection:
		for (cur, sysinfo, fio_stats, samples) in stats:
			job = sysinfo['Job']
			config = job_config_get(job)
			for c in config:
				if not c in unique_configs:
					unique_configs.append(c)

	#
	#  The following are config values that we may be
	#  interested in labelling
	#
	sysinfokeys = [ 'Blocksize', 'Filesystem', 'Device', 'Hardware',
		       	'Job', 'Date', 'Kernel-release', 'Filesize', 'Processor', 'IO Scheduler' ]
	#
	#  Step #1, find all the different sysinfo fields that change
	#  across all the collections and tests
	#
	sysinfovals = {}
	collection_fields_vary = []
	for config in unique_configs:
		for stats in stats_collection:
			for (cur, sysinfo, fio_stats, samples) in stats:
				if config in job_config_get(sysinfo['Job']):
					for c in sysinfokeys:
						try:
							if not sysinfo[c] in sysinfovals[c]:
								sysinfovals[c].append(sysinfo[c])
						except:
							sysinfovals[c] = [ sysinfo[c] ]
	for s in sysinfovals:
		if len(sysinfovals[s]) > 1:
			collection_fields_vary.append(s)
	#
	#  Step #2, gather the data
	#
	for config in unique_configs:
		sysinfovals = {}
		collection = []
		#
		#  find all the different sysinfo fields that change and don't
		#  change across the tests
		#
		for stats in stats_collection:
			for (cur, sysinfo, fio_stats, samples) in stats:
				if config in job_config_get(sysinfo['Job']):
					for c in sysinfokeys:
						try:
							if not sysinfo[c] in sysinfovals[c]:
								sysinfovals[c].append(sysinfo[c])
						except:
							sysinfovals[c] = [sysinfo[c]]
		sysinfo_fields_vary = []
		sysinfo_fields_static = []

		for s in sysinfovals:
			if len(sysinfovals[s]) > 1:
				if not s in collection_keys:
					sysinfo_fields_vary.append(s)
			else:
				sysinfo_fields_static.append(s)

		label = config['Label']
		units = config['Units']
		scale = float(config['Scale'])
		jsonpath = config['JsonPath']

		subtitle = []
		for f in sysinfo_fields_static:
			subtitle.append(sysinfo[f])

		for stats in stats_collection:
			yvalues = []
			xlabels = []
			for (cur, sysinfo, fio_stats, samples) in stats:
				if config in job_config_get(sysinfo['Job']):
					val = get_json_val(fio_stats, jsonpath)
					yvalues.append(val / scale)
					xlabel = ""
					for f in sysinfo_fields_vary:
						xlabel += sysinfo[f] + ", "

					xlabels.append(clean_label(xlabel))

					for c in sysinfokeys:
						try:
							if not sysinfo[c] in sysinfovals[c]:
								sysinfovals[c].append(sysinfo[c])
						except:
							sysinfovals[c] = [sysinfo[c]]
				xlabel = []
				for f in collection_keys:
					xlabel.append(sysinfo[f])
			#
			# Add them to the subplot collection
			#
			if len(xlabels) > 0:
				collection.append((array_to_str(subtitle), xlabels, yvalues, array_to_str(xlabel)))

		graph_func(html_subdirs, root_path, keys, config, collection)

#
#  Load in fio-stats.json and sysinfo.log for the given tests
#
def collect_stats(path):
	stats = []
	for cur, _dirs, files in os.walk(path):
		head, tail = os.path.split(cur)
		if tail == "stats":
			sysinfo = sysinfo_load(os.path.join(cur, 'sysinfo.log'))
			job_config_get(sysinfo['Job'])
			samples = samples_load(cur)
			try:
				f = open(os.path.join(cur, 'fio-stats.json'), 'r')
				fio_stats = json.load(f)
				f.close()
				stats.append((cur, sysinfo, fio_stats, samples))
			except:
				print "Failed to read fio-stats.json for " + cur

	return stats

#
#  Given a set of criteria, find all the gathered stats that match
#
def find_matching_stats(stats, criteria):
	matches = []
	for (cur, sysinfo, fio_stats, samples) in stats:
		keep = True
		for (key, values) in criteria:
			if values != []:
				found = False
				for value in values:
					if key in sysinfo and sysinfo[key] == value:
						found = True
						break
				keep = keep and found
		if keep:
			matches.append((cur, sysinfo, fio_stats, samples))

	return matches

def find_unique_key_value_stats(stats, criteria, key):
	values = []
	matches = find_matching_stats(stats, criteria)
	for (cur, sysinfo, fio_stats, samples) in matches:
		if not sysinfo[key] in values:
			values.append(sysinfo[key])

	print "Found %d unique stats" % len(values)
	return values

#
#  Load samples given a path and a regex pattern for the
#  raw data name.  Loads fio raw samples.
#
def samples_load_by_pattern(path, pattern):
	regex = re.compile(pattern)
	values = []
	for l in sorted(os.listdir(path)):
		x = []
		y = []
		for m in [regex.search(l)]:
			if m:
				filename = os.path.join(path, l)
				if filename[-3:] == '.gz':
					f = gzip.open(filename)
				else:
					f = open(filename, 'r')

				lines = f.readlines()
				f.close()
				x = [ int(col.split(',')[0]) for col in lines]
				y = [ int(col.split(',')[1]) for col in lines]
				values.append((x,y))
	return values

#
#  Load samples from file from a given path
#
def samples_load(path):
	print "Loading samples for " + path
	samples = {}
	for (key, title, pattern, name) in samples_info:
		samples[key] = samples_load_by_pattern(path, pattern)

	return samples

def graph_samples(html_subdirs, root_path, path, key, stats, keys, title):
	jobs = []
	print "Processing " + key

	#
	# Find all unique jobs
	#
	for (cur, sysinfo, fio_stats, samples) in stats:
		if sysinfo['Job'] not in jobs:
			jobs.append(sysinfo['Job'])
	#
	# Process data from each unique job:
	#
	for job in jobs:
		data = []
		titles = []
		for (cur, sysinfo, fio_stats, samples) in stats:
			if sysinfo['Job'] == job:
				data.append((samples[key], sysinfo))
				titles.append(sysinfo_keys_to_label(sysinfo, keys))
		#
		# Sort based on the chart titles
		#
		indexes = np.argsort(titles)
		titles = [ titles[i] for i in indexes ]
		data = [ data[i] for i in indexes ]

		new_path = os.path.join(path, job)
		mkdirp(new_path)

		if len(data):
			plot_samples_raw_fio_data(new_path, keys, title, data)
			hist_samples_raw_fio_data(new_path, keys, title, data)
			html_subdirs.append(generate_html([], root_path, new_path, job, job))
		else:
			print "No relevant data can be found"

#
#  break stats into collections based on the group_by_keys
#
def compare_stats_ordered_traverse(graph_func, stats, group_by_keys, collection_keys, traversed_stats):
	if (len(group_by_keys) <= 1):
		sortstats = sorted(stats, key = lambda d: d[1][group_by_keys[0]])
		traversed_stats.append(sortstats)
		return
	#
	#  For all key values, partition into "collections"
	#  where the key value is identical and sub-divide
	#  or graph on next key value
	#
	k = group_by_keys[0]
	suborder_by_keys = group_by_keys[1:len(group_by_keys)]

	if not k in collection_keys:
		compare_stats_ordered_traverse(graph_func, stats, suborder_by_keys, collection_keys, traversed_stats)
	else:
		values = []
		for (cur, sysinfo, fio_stats, samples) in stats:
			if not sysinfo[k] in values:
				values.append(sysinfo[k])

		for v in values:
			substats = []
			for (cur, sysinfo, fio_stats, samples) in stats:
				if sysinfo[k] == v:
					substats.append((cur, sysinfo, fio_stats, samples))

			compare_stats_ordered_traverse(graph_func, substats, suborder_by_keys, collection_keys, traversed_stats)

#
#  group up data by group_by_keys and then graph it using graph_func
#
def compare_stats_ordered(html_subdirs, path, graph_func, stats, group_by_keys, collection_keys):
	traversed_stats = []
	compare_stats_ordered_traverse(graph_func, stats, group_by_keys, collection_keys, traversed_stats)
	graph_stats(html_subdirs, path, graph_func, traversed_stats, collection_keys)

#
#  generate a html file containing links to graphs
#
def generate_html(html_subdirs, root_path, sub_path, html_file, title):
	page = '''
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <meta content="text/html; charset=ISO-8859-1"
 http-equiv="content-type">
  <title>''' + title + '''</title>
  <h2><center>''' + title + '''</center></h2>'''
	page += '''
</head>
<body>
<center>
  <table width=100% cellspacing="2">
'''
	colors = [ "#ffffff", "#e0f0ff" ]
	i = 0
	for (t, f) in sorted(html_subdirs, key=lambda d: d[0]):
		common = os.path.commonprefix([sub_path, f])
		relname = f[len(common)+1:]
		page += '\n    <tr><td><center><a href="' + relname + '">' + t + '</a></center></td></tr>'
	for f in sorted(os.listdir(sub_path)):
		(filename, extension) = os.path.splitext(f)
		if extension == ".png":
			page += '\n <tr><td bgcolor="' + colors[i % 2] + '"><center><img src=' + f + '></center></td></tr>'
			i += 1
	page += '''
  </table>
</center>
</body>
</html>'''
	mkdirp(sub_path)
	filename = os.path.join(sub_path, "index.html")
	f = open(filename, 'w')
	f.write(page)
	f.close()
	return (title, filename)

#
#  Histogram of fs + tests on a given day
#
def daily_tests_histogram(html_dirs, root_path, date):
	tests = find_matching_stats(stats, 
		[
			('Date', date),
			('Job', []),
			('Filesystem', []),
			('Kernel-release', []),
			('IO Scheduler', [])
		])
	#
	# Compare filesystems
	#
	newpath = os.path.join(root_path, 'daily-fio-histograms-by-filesystem')
	mkdirp(newpath)
	html_subdirs = []
	compare_stats_ordered(html_subdirs, newpath, bar_horiz, tests,
		['IO Scheduler', 'Job', 'Filesystem', 'Date' ], ['Job'])
	html_dirs.append(generate_html(html_subdirs, root_path, newpath, 'daily-tests-histogram', 'Fio Test Metrics (by metric) ' + ', '.join(date)))

#
#  Samples of fs on a given day
#
def daily_tests_samples(html_dirs, root_path, date):
	tests = find_matching_stats(stats,
		[
			('Date', date),
			('Job', []),
			('Filesystem', []),
			('Kernel-release', [])
		])
	keys = keys_variant_collection([tests])
	#
	#  Process raw samples from fio logs
	#
	for (key, title, pattern, name) in samples_info:
		new_path = os.path.join(root_path, name)
		mkdirp(new_path)
		html_subdirs = []
		graph_samples(html_subdirs, root_path, new_path, key, tests, keys, title)
		html_dirs.append(generate_html(html_subdirs, root_path, new_path, name, title + ": " + ', '.join(date)))

#
#  Plot trends over time
#
def overtime_trends(html_dirs, root_path):
	newpath = os.path.join(root_path, 'overtime-trends')
	mkdirp(newpath)

	kernel_release = []
	file_systems = []
	date = []

	jobs = find_unique_key_value_stats(stats,
		[
			('Date', date),
			('Job', []),
			('Filesystem', file_systems),
			('Kernel-release', kernel_release)
		], 'Job')

	html_subdirs = []
	for job in jobs:
		print "Generating trends for job " + job
		jobpath = os.path.join(newpath, job)
		mkdirp(jobpath)
		tests = find_matching_stats(stats, 
			[
				('Date', date),
				('Job', [job]),
				('Filesystem', file_systems),
				('Kernel-release', kernel_release )
			])
		compare_stats_ordered(html_subdirs, jobpath, plot_horiz, tests,
			[ 'Job', 'Filesystem', 'Date'], ['Filesystem', 'Job'])
		html_subdirs.append(generate_html([], newpath, jobpath, 'overtime-trends', job + ": " + 'trends over time'))

	html_dirs.append(generate_html(html_subdirs, newpath, newpath, 'overtime-trends', 'All trends over time'))

#
#  Gather up stats
#
stats = []

print "Reading data.."
for i in range(1, len(sys.argv)):
	stats += collect_stats(sys.argv[i])

tmpdir = "/tmp/test"
print "results in " + tmpdir

html_dirs = []
daily_tests_histogram(html_dirs, tmpdir, [ '2014-07-01', '2014-07-02'])
daily_tests_samples(html_dirs, tmpdir, ['2014-07-01', '2014-07-02'])
overtime_trends(html_dirs, tmpdir)

generate_html(html_dirs, tmpdir, tmpdir, 'results', 'All results')
